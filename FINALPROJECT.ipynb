{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FINALPROJECT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brianvoha/Lyric-based-Classification-of-Musical-Genres/blob/master/FINALPROJECT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mtR3mCkuXBTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install langdetect"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jyT7G0IbjCp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install whatthelang"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6CXLb8i2NFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DONT RUN\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from whatthelang import WhatTheLang\n",
        "\n",
        "songList = pd.read_csv(\"lyrics.csv\", engine='python', error_bad_lines=False, encoding='utf-8')\n",
        "\n",
        "# replace '\\n' with space\n",
        "songList = songList.replace({'\\n': ' '}, regex=True)\n",
        "\n",
        "# Clean up the dataset (from Lyrics Genre Analysis - Machine Learning)\n",
        "songList['lyrics'] = songList['lyrics'].str.replace(r'[^\\w\\s]+', '')\n",
        "songList['lyrics'] = songList['lyrics'].str.replace(r'^\\s+$', '')\n",
        "songList['lyrics'] = songList['lyrics'].str.replace(r'^[\\s\\d\\s]+$', '')\n",
        "\n",
        "# remove all songs with <13 characters (Includes all the Instrumental songs)\n",
        "songList=songList[songList.lyrics.apply(lambda x: len(str(x))>=13)]\n",
        "print(str(len(songList))+\" rows left.\")\n",
        "\n",
        "# Remove duplicate lyrics fields, keep the first entry\n",
        "songList.drop_duplicates(subset =\"lyrics\", keep = 'first', inplace = True)\n",
        "\n",
        "print(str(len(songList))+\" rows left.\") \n",
        "\n",
        "# Remove songs with 'Not Available' or 'Other' as their genres\n",
        "songList = songList[songList['genre']!='Not Available'].dropna()\n",
        "songList = songList[songList['genre']!='Other'].dropna()\n",
        "print(str(len(songList))+\" rows left.\")\n",
        "\n",
        "# Remove songs that are not in english\n",
        "wtl = WhatTheLang()\n",
        "songList=songList[songList.lyrics.apply(lambda x: wtl.predict_lang(str(x))==\"en\")]\n",
        "print(str(len(songList))+\" rows left.\")\n",
        "\n",
        "songList['word_count'] = songList['lyrics'].str.split().str.len()\n",
        "songList.head()\n",
        "\n",
        "#remove all songs with <100 words or >1000 words\n",
        "df_clean = songList[songList['word_count'] >= 100]\n",
        "df_clean = songList[songList['word_count'] <= 1000]\n",
        "df_clean['word_count'].groupby(df_clean['genre']).describe()\n",
        "print(str(len(df_clean))+\" rows left.\")\n",
        "\n",
        "df_clean.to_csv(\"clean.csv\", encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0NYWUaDkDEv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#DONT RUN\n",
        "import random\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eFOfqQyZnxTV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#UPLOAD CLEAN.CSV AND START HERE\n",
        "import pandas as pd\n",
        "\n",
        "songList = pd.read_csv(\"clean.csv\", engine='python', error_bad_lines=False, encoding='utf-8')\n",
        "index = []\n",
        "columns = ['index','song','year','artist','genre','lyrics']\n",
        "final = pd.DataFrame(index=index,columns=columns)\n",
        "a = set(songList['genre'])\n",
        "\n",
        "# remove 5 genres with the least amount of songs\n",
        "a.remove('Electronic')\n",
        "a.remove('Folk')\n",
        "a.remove('Indie')\n",
        "a.remove('Jazz')\n",
        "a.remove('R&B')\n",
        "\n",
        "# take a random sample of 13353 for each of the remaining genres and put them into \n",
        "for genre in a:\n",
        "    temp = songList[songList['genre'] == genre].sample(n=13353,random_state = 1)\n",
        "    final = pd.concat([final,temp])\n",
        "print(len(final))\n",
        "final.to_csv(\"balanced.csv\", encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWLjRpgRGvJ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DONT RUN\n",
        "# remove stopwords from final\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# stop_words = stopwords.words('english')\n",
        "# tokenized = final['lyrics'].apply(lambda x: x.split())\n",
        "# tokenized = tokenized.apply(lambda x: [item for item in x if item not in stop_words])\n",
        "# detokenized_doc = [] \n",
        "# for i in range(len(final)): \n",
        "#     t = ' '.join(tokenized[i]) \n",
        "#     detokenized_doc.append(t) \n",
        "# final['lyrics'] = detokenized_doc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17C9sHolhJki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from fastai.text import *\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# split data into train/val and test sets\n",
        "final, test = train_test_split(final, test_size=0.2, random_state=1)\n",
        "print(final.shape)\n",
        "print(test.shape)\n",
        "final = final[~final.isna().any(axis=1)]\n",
        "# split train/val ito train and val sets\n",
        "final['train'] = np.random.choice(a=[True,False], size=len(final), p=[0.8,0.2])\n",
        "final.groupby('train').size()\n",
        "df_full = final.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7z46nRTTfC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create the language model and classfier model data bunches\n",
        "data_lm2 = TextLMDataBunch.from_df(path='.', \n",
        "                             train_df=final[final['train']], \n",
        "                             valid_df=final[~final['train']], \n",
        "                             text_cols = 'lyrics',\n",
        "                            )\n",
        "data2 = TextClasDataBunch.from_df(path='.', \n",
        "                             train_df=final[final['train']], \n",
        "                             valid_df=final[~final['train']], \n",
        "                             text_cols = 'lyrics',\n",
        "                             label_cols = 'genre',\n",
        "                             vocab = data_lm2.train_ds.vocab, bs=32\n",
        "                            )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XLsYYCsWd_1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save our data bunches so we don't have to create them every time\n",
        "data_lm2.save('data_lm2_export.pkl')\n",
        "data2.save('data2_export.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NULQxZELWsLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load in the data bunches\n",
        "data_lm2 = load_data('.', 'data_lm2_export.pkl')\n",
        "data2 = load_data('.', 'data2_export.pkl', bs=16)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy1VVU-9W5L3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#start fine-tuning the pre-trained model to fit our dataset\n",
        "learn2 = language_model_learner(data_lm2, AWD_LSTM, drop_mult=0.5)\n",
        "learn2.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xbB4IpLubLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn2.unfreeze()\n",
        "learn2.fit_one_cycle(1, 1e-3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o24jsBt6ysSu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn2.save_encoder('ft_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx8-Mx1MGVub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn2 = text_classifier_learner(data2, AWD_LSTM, drop_mult=0.5)\n",
        "learn2.load_encoder('ft_enc')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scidgRaqGlq4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data2.show_batch()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QErJ-fovGqQJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn2.fit_one_cycle(1, 1e-2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYvJf_p0LoKn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn2.freeze_to(-2)\n",
        "learn2.fit_one_cycle(1, slice(5e-3/2., 5e-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGmzbB_fLyk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learn2.unfreeze()\n",
        "learn2.fit_one_cycle(1, slice(2e-3/100, 2e-3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wlJaEN2lCoe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp2 = ClassificationInterpretation.from_learner(learn2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJ6YC7RKmw5I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp2.most_confused(slice_size=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kooqZ1A_nAP3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "interp2.plot_confusion_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Od4keoMuw5hP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lyric = \"MidnightYou come and pick me up, no headlights Long driv Could end in burning flames or paradise Fade into view, oh It's been a while since I have even heard from you And I should just tell you to leave, 'cause I Know exactly where it leads, but I Watch us go 'round and 'round each time You got that James Dean daydream look in your eye And I got that red lip classic thing that you like And when we go crashing down, we come back every time 'Cause we never go out of style, we never go out of style You got that long hair, slicked back, white t-shirt And I got that good girl faith and a tight little skirt And when we go crashing down, we come back every time 'Cause we never go out of style, we never go out of style\"\n",
        "print(str(learn2.predict(lyric)[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGejPmu-jj0Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for index, row in test.iterrows():\n",
        "  test.at[index,'predicted'] = str(learn2.predict(row.lyrics)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OelbCj5v1V_k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seriesObj = test.apply(lambda x: True if x['predicted'] == x['genre'] else False , axis=1)\n",
        "# Count number of True in series\n",
        "numOfRows = len(seriesObj[seriesObj == True].index)\n",
        "\n",
        "print(\"There are \"+str(numOfRows)+\" correctly identified songs.\")\n",
        "print(\"Accuracy: \"+str(numOfRows/float(len(test))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQgWnrBgPlet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install palettable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRMg1bdQ364B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sn\n",
        "from palettable.colorbrewer.sequential import YlOrBr_4\n",
        "from matplotlib.colors import ListedColormap\n",
        "import palettable\n",
        "\n",
        "# test = pd.read_csv(\"predictFINAL.csv\", engine='python', error_bad_lines=False, encoding='utf-8')\n",
        "arr_actual = test['genre'].astype(str).values.tolist()\n",
        "arr_predicted = test['predicted'].astype(str).values.tolist()\n",
        "\n",
        "# confusion_matrix(arr_actual, arr_predicted)\n",
        "y_actu = pd.Series(arr_actual, name='Actual')\n",
        "y_pred = pd.Series(arr_predicted, name='Predicted')\n",
        "df_confusion = pd.crosstab(test['genre'], test['predicted'], rownames=['Actual'], colnames=['Predicted'])\n",
        "plt.figure(figsize = (6,5))\n",
        "cmap = ListedColormap(palettable.colorbrewer.sequential.YlOrBr_4.mpl_colors)\n",
        "sn.heatmap(df_confusion, annot=True, fmt='g', cmap=cmap,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiBTKbRWBq0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test.to_csv(\"predict.csv\", encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHG2wO4rqYzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Tim's code for Naive Bayes, Logistic Regression, KNN, SVC\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "import seaborn as sn\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop = set(stopwords.words('english'))\n",
        "\n",
        "lyrics = []\n",
        "genre = []\n",
        "songList = pd.read_csv(\"balanced.csv\", engine='python', error_bad_lines=False, encoding='utf-8')\n",
        "for row in songList.itertuples():\n",
        "    tlyrics = \" \".join([lemmatizer.lemmatize(x.lower()) for x in row.lyrics.split() if x not in stop])\n",
        "\n",
        "    lyrics.append(tlyrics)\n",
        "    genre.append(row.genre)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer( max_features=10000)\n",
        "x = tfidf_vectorizer.fit_transform(lyrics)\n",
        "\n",
        "xtrain, xval, ytrain, yval = train_test_split(x, genre, test_size=0.2, random_state=9)\n",
        "#y = multilabel_binarizer.inverse_transform(songList['genre'])\n",
        "#print(x.shape)\n",
        "#print(y)\n",
        "knn = KNeighborsClassifier(n_neighbors=30)\n",
        "knn.fit(xtrain,ytrain)\n",
        "nb = MultinomialNB()\n",
        "nb.fit(xtrain,ytrain)\n",
        "lg = LogisticRegression()\n",
        "lg.fit(xtrain,ytrain)\n",
        "pred = lg.predict(xval)\n",
        "svm = LinearSVC()\n",
        "svm.fit(xtrain,ytrain)\n",
        "\n",
        "a = list(set(genre))\n",
        "print(a)\n",
        "print(confusion_matrix(yval,pred,labels= a))\n",
        "df_confusion = pd.crosstab(yval, pred, rownames=['Actual'], colnames=['Predicted'])\n",
        "plt.figure(figsize = (6,5))\n",
        "sn.heatmap(df_confusion, annot=True, fmt='g', cmap='Blues')\n",
        "print(\"nb:\",nb.score(xval,yval))\n",
        "print(\"knn:\",knn.score(xval,yval))\n",
        "print(\"lg:\",lg.score(xval,yval))\n",
        "print(\"svm:\",svm.score(xval,yval))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JvAFsHIlNCEi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "songList = pd.read_csv(\"clean.csv\", engine='python', error_bad_lines=False, encoding='utf-8')\n",
        "\n",
        "for genre in set(songList['genre']):\n",
        "    print(genre,len(songList[songList['genre']==genre]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}